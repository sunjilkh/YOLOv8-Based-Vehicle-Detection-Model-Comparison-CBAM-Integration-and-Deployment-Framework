{"cells":[{"cell_type":"markdown","id":"a241b5bf","metadata":{"id":"a241b5bf"},"source":["# üöÄ YOLOv8 with CBAM Attention - Google Colab Training\n","## Bangladesh Road Vehicle Detection\n","\n","**Model**: YOLOv8n with CBAM (Convolutional Block Attention Module)\n","\n","**Dataset**: 10 classes - bicycle, bus, car, CNG, auto, bike, Multi-Class, rickshaw, truck, van\n","\n","**Hardware**: Google Colab T4 GPU (Free)\n","\n","**Expected Training Time**: 3-4 hours for 100 epochs\n","\n","---\n","\n","### üìã Instructions:\n","1. Run cells in order from top to bottom\n","2. Don't skip any cells\n","3. Wait for each cell to complete before running the next\n","4. Check for ‚úì marks indicating success"]},{"cell_type":"markdown","id":"9b8b16bf","metadata":{"id":"9b8b16bf"},"source":["## üìÅ Step 1: Mount Google Drive\n","\n","This will ask for permission to access your Google Drive."]},{"cell_type":"code","execution_count":1,"id":"c54208e2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c54208e2","executionInfo":{"status":"ok","timestamp":1762496717753,"user_tz":-360,"elapsed":29533,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"2d0c29c4-cec8-4165-ee3e-12fad2b4bfdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","======================================================================\n","‚úì Google Drive mounted successfully!\n","======================================================================\n"]}],"source":["from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","print('\\n' + '='*70)\n","print('‚úì Google Drive mounted successfully!')\n","print('='*70)"]},{"cell_type":"markdown","id":"10c6f018","metadata":{"id":"10c6f018"},"source":["## ‚öôÔ∏è Step 2: Install Dependencies\n","\n","Install Ultralytics YOLOv8 and required packages."]},{"cell_type":"code","execution_count":2,"id":"90cee403","metadata":{"id":"90cee403","executionInfo":{"status":"ok","timestamp":1762496725015,"user_tz":-360,"elapsed":7255,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}}},"outputs":[],"source":["%%capture\n","# Install ultralytics (output suppressed for cleaner display)\n","!pip install ultralytics==8.0.200 --quiet\n","\n","print('Installing dependencies...')"]},{"cell_type":"code","execution_count":3,"id":"e5ad444c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5ad444c","executionInfo":{"status":"ok","timestamp":1762496738579,"user_tz":-360,"elapsed":13554,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"b57c7e9d-25af-4f6e-e01f-640e14186e82"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","ENVIRONMENT VERIFICATION\n","======================================================================\n","‚úì Python version: 3.12.12\n","‚úì PyTorch version: 2.8.0+cu126\n","‚úì Ultralytics version: 8.0.200\n","‚úì CUDA available: True\n","‚úì CUDA version: 12.6\n","‚úì GPU: Tesla T4\n","‚úì GPU Memory: 14.7 GB\n","======================================================================\n"]}],"source":["# Verify installation\n","import sys\n","import torch\n","import ultralytics\n","from ultralytics import YOLO\n","\n","print('='*70)\n","print('ENVIRONMENT VERIFICATION')\n","print('='*70)\n","print(f'‚úì Python version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}')\n","print(f'‚úì PyTorch version: {torch.__version__}')\n","print(f'‚úì Ultralytics version: {ultralytics.__version__}')\n","print(f'‚úì CUDA available: {torch.cuda.is_available()}')\n","\n","if torch.cuda.is_available():\n","    print(f'‚úì CUDA version: {torch.version.cuda}')\n","    print(f'‚úì GPU: {torch.cuda.get_device_name(0)}')\n","    print(f'‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n","else:\n","    print('‚ö† WARNING: CUDA not available. Training will be very slow on CPU!')\n","\n","print('='*70)"]},{"cell_type":"markdown","id":"25184a8d","metadata":{"id":"25184a8d"},"source":["## üìÇ Step 3: Configure Paths\n","\n","**‚ö†Ô∏è IMPORTANT**: Update `DRIVE_PROJECT_PATH` to match your Google Drive folder structure."]},{"cell_type":"code","execution_count":4,"id":"0b42ce65","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0b42ce65","executionInfo":{"status":"ok","timestamp":1762496739816,"user_tz":-360,"elapsed":1229,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"92925035-3fa6-4a32-c48e-b105125594b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","PATH CONFIGURATION\n","======================================================================\n","Drive project: /content/drive/MyDrive/YOLOv8 Traffic\n","Dataset path: /content/drive/MyDrive/YOLOv8 Traffic/dataset\n","Data config: /content/drive/MyDrive/YOLOv8 Traffic/data.yaml\n","Working dir: /content/yolov8_cbam\n","Results dir: /content/runs\n","======================================================================\n","\n","Verifying files...\n","‚úì data.yaml found\n"]}],"source":["import os\n","from pathlib import Path\n","\n","# ============================================================================\n","# ‚ö†Ô∏è UPDATE THIS PATH TO YOUR GOOGLE DRIVE FOLDER\n","# ============================================================================\n","DRIVE_PROJECT_PATH = '/content/drive/MyDrive/YOLOv8 Traffic'\n","\n","# Dataset path (from data.yaml)\n","DATASET_PATH = f'{DRIVE_PROJECT_PATH}/dataset'\n","\n","# File paths\n","DATA_YAML = f'{DRIVE_PROJECT_PATH}/data.yaml'\n","PRETRAINED_WEIGHTS = 'yolov8n.pt'  # Will be downloaded automatically\n","\n","# Output paths (on Colab, will be copied to Drive later)\n","COLAB_WORK_DIR = '/content/yolov8_cbam'\n","RESULTS_DIR = '/content/runs'\n","\n","# Create working directory\n","os.makedirs(COLAB_WORK_DIR, exist_ok=True)\n","\n","# Verify paths\n","print('='*70)\n","print('PATH CONFIGURATION')\n","print('='*70)\n","print(f'Drive project: {DRIVE_PROJECT_PATH}')\n","print(f'Dataset path: {DATASET_PATH}')\n","print(f'Data config: {DATA_YAML}')\n","print(f'Working dir: {COLAB_WORK_DIR}')\n","print(f'Results dir: {RESULTS_DIR}')\n","print('='*70)\n","\n","# Verify critical files exist\n","print('\\nVerifying files...')\n","if os.path.exists(DATA_YAML):\n","    print(f'‚úì data.yaml found')\n","else:\n","    print(f'‚úó ERROR: data.yaml not found at {DATA_YAML}')\n","    print('  Please check your DRIVE_PROJECT_PATH setting above!')\n","    raise FileNotFoundError(f'data.yaml not found at {DATA_YAML}')"]},{"cell_type":"markdown","id":"518bc879","metadata":{"id":"518bc879"},"source":["## üß¨ Step 4: Define CBAM Module\n","\n","This cell defines the CBAM (Convolutional Block Attention Module) with comprehensive error handling."]},{"cell_type":"code","execution_count":5,"id":"7251d5a0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7251d5a0","executionInfo":{"status":"ok","timestamp":1762496741673,"user_tz":-360,"elapsed":1855,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"88cbd454-f431-402e-aaa0-54572cb240de"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","CBAM MODULE DEFINED\n","======================================================================\n","Testing CBAM module...\n","  CBAM initialized: c1=64, reduction_ratio=16, shortcut=True\n","‚úì CBAM test passed!\n","  Input shape: (1, 64, 32, 32)\n","  Output shape: (1, 64, 32, 32)\n","  Parameters: 610\n","  Device: GPU\n","======================================================================\n"]}],"source":["\"\"\"\n","CBAM: Convolutional Block Attention Module\n","Reference: Woo et al., ECCV 2018\n","Implementation optimized for Google Colab with T4 GPU\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from typing import Optional\n","import warnings\n","\n","\n","class ChannelAttention(nn.Module):\n","    \"\"\"Channel Attention Module - focuses on 'what' is meaningful.\"\"\"\n","\n","    def __init__(self, in_channels: int, reduction_ratio: int = 16):\n","        super(ChannelAttention, self).__init__()\n","\n","        if in_channels <= 0:\n","            raise ValueError(f\"in_channels must be positive, got {in_channels}\")\n","        if reduction_ratio <= 0:\n","            raise ValueError(f\"reduction_ratio must be positive, got {reduction_ratio}\")\n","\n","        self.reduced_channels = max(in_channels // reduction_ratio, 1)\n","\n","        # Shared MLP using 1x1 convolutions (more efficient than FC)\n","        self.fc1 = nn.Conv2d(in_channels, self.reduced_channels, 1, bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Conv2d(self.reduced_channels, in_channels, 1, bias=False)\n","\n","        # Pooling layers\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","\n","        # Initialize weights\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        \"\"\"Kaiming initialization for ReLU activation.\"\"\"\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        if not isinstance(x, torch.Tensor):\n","            raise TypeError(f\"Expected torch.Tensor, got {type(x)}\")\n","        if x.dim() != 4:\n","            raise ValueError(f\"Expected 4D input (B,C,H,W), got shape {x.shape}\")\n","\n","        # Average pooling branch\n","        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n","\n","        # Max pooling branch\n","        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n","\n","        # Combine and apply sigmoid\n","        out = torch.sigmoid(avg_out + max_out)\n","\n","        return out\n","\n","\n","class SpatialAttention(nn.Module):\n","    \"\"\"Spatial Attention Module - focuses on 'where' is informative.\"\"\"\n","\n","    def __init__(self, kernel_size: int = 7):\n","        super(SpatialAttention, self).__init__()\n","\n","        if kernel_size <= 0 or kernel_size % 2 == 0:\n","            raise ValueError(f\"kernel_size must be positive and odd, got {kernel_size}\")\n","\n","        padding = kernel_size // 2\n","\n","        # 7x7 convolution to aggregate channel information\n","        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n","\n","        # Initialize weights\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        \"\"\"Xavier initialization for sigmoid activation.\"\"\"\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.xavier_normal_(m.weight)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        if not isinstance(x, torch.Tensor):\n","            raise TypeError(f\"Expected torch.Tensor, got {type(x)}\")\n","        if x.dim() != 4:\n","            raise ValueError(f\"Expected 4D input (B,C,H,W), got shape {x.shape}\")\n","\n","        # Aggregate channel information\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","\n","        # Concatenate and apply convolution + sigmoid\n","        concat = torch.cat([avg_out, max_out], dim=1)\n","        out = torch.sigmoid(self.conv(concat))\n","\n","        return out\n","\n","\n","class CBAM(nn.Module):\n","    \"\"\"\n","    Complete CBAM module combining Channel and Spatial Attention.\n","\n","    Args:\n","        c1: Input channels (will be auto-scaled by YOLOv8)\n","        c2: Output channels (ignored, CBAM preserves channels)\n","        reduction_ratio: Channel reduction ratio for efficient attention\n","        kernel_size: Kernel size for spatial attention convolution\n","        shortcut: Whether to use residual connection\n","\n","    Note: YOLOv8 automatically scales c1 based on model variant (n/s/m/l/x).\n","          For 'n' variant with scale [0.33, 0.25, 1024], c1 will be scaled by 0.25.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        c1: int,\n","        c2: Optional[int] = None,\n","        reduction_ratio: int = 16,\n","        kernel_size: int = 7,\n","        shortcut: bool = True\n","    ):\n","        super(CBAM, self).__init__()\n","\n","        # Validate inputs\n","        if not isinstance(c1, int) or c1 <= 0:\n","            raise ValueError(f\"c1 must be a positive integer, got {c1} (type: {type(c1)})\")\n","        if reduction_ratio <= 0:\n","            raise ValueError(f\"reduction_ratio must be positive, got {reduction_ratio}\")\n","        if kernel_size <= 0 or kernel_size % 2 == 0:\n","            raise ValueError(f\"kernel_size must be positive and odd, got {kernel_size}\")\n","\n","        # CBAM doesn't change channel dimensions - it's an attention mechanism\n","        # c2 is provided by YOLOv8 parser but should equal c1\n","        if c2 is not None and c2 != c1:\n","            warnings.warn(\n","                f\"CBAM is attention module - doesn't change channels. \"\n","                f\"c2={c2} ignored, output will have c1={c1} channels.\"\n","            )\n","\n","        # Store configuration\n","        self.c1 = c1  # Input channels (already scaled by YOLOv8)\n","        self.c2 = c1  # Output channels (same as input for attention)\n","        self.shortcut = shortcut\n","        self.reduction_ratio = reduction_ratio\n","\n","        # Build attention modules with ACTUAL input channels\n","        self.channel_attention = ChannelAttention(c1, reduction_ratio)\n","        self.spatial_attention = SpatialAttention(kernel_size)\n","\n","        # Debug info (will be printed during model construction)\n","        print(f\"  CBAM initialized: c1={c1}, reduction_ratio={reduction_ratio}, shortcut={shortcut}\")\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Apply CBAM attention to input tensor.\n","\n","        Args:\n","            x: Input tensor of shape (B, C, H, W)\n","\n","        Returns:\n","            Attended tensor of shape (B, C, H, W)\n","        \"\"\"\n","        # Comprehensive input validation\n","        if not isinstance(x, torch.Tensor):\n","            raise TypeError(f\"Expected torch.Tensor input, got {type(x)}\")\n","\n","        if x.dim() != 4:\n","            raise ValueError(f\"Expected 4D input (B,C,H,W), got {x.dim()}D with shape {x.shape}\")\n","\n","        # Verify channel count matches initialization\n","        actual_channels = x.size(1)\n","        if actual_channels != self.c1:\n","            raise ValueError(\n","                f\"Channel mismatch! CBAM was initialized with c1={self.c1} channels, \"\n","                f\"but received input with {actual_channels} channels. \"\n","                f\"Input shape: {x.shape}. \"\n","                f\"This usually means the YAML architecture has incorrect channel specifications.\"\n","            )\n","\n","        # Check for invalid values\n","        if torch.isnan(x).any():\n","            raise RuntimeError(f\"Input contains NaN values! Shape: {x.shape}\")\n","        if torch.isinf(x).any():\n","            raise RuntimeError(f\"Input contains Inf values! Shape: {x.shape}\")\n","\n","        # Store identity for residual connection\n","        identity = x\n","\n","        # Apply channel attention: emphasize important channels\n","        x = x * self.channel_attention(x)\n","\n","        # Apply spatial attention: emphasize important spatial regions\n","        x = x * self.spatial_attention(x)\n","\n","        # Add residual connection (helps gradient flow and preserves information)\n","        if self.shortcut:\n","            x = x + identity\n","\n","        # Final validation\n","        if torch.isnan(x).any():\n","            raise RuntimeError(\"CBAM output contains NaN values!\")\n","        if torch.isinf(x).any():\n","            raise RuntimeError(\"CBAM output contains Inf values!\")\n","\n","        return x\n","\n","\n","# Quick test\n","print('='*70)\n","print('CBAM MODULE DEFINED')\n","print('='*70)\n","print('Testing CBAM module...')\n","\n","try:\n","    test_cbam = CBAM(c1=64)\n","    test_input = torch.randn(1, 64, 32, 32)\n","\n","    if torch.cuda.is_available():\n","        test_cbam = test_cbam.cuda()\n","        test_input = test_input.cuda()\n","\n","    with torch.no_grad():\n","        test_output = test_cbam(test_input)\n","\n","    assert test_output.shape == test_input.shape\n","    assert not torch.isnan(test_output).any()\n","    assert not torch.isinf(test_output).any()\n","\n","    print(f'‚úì CBAM test passed!')\n","    print(f'  Input shape: {tuple(test_input.shape)}')\n","    print(f'  Output shape: {tuple(test_output.shape)}')\n","    print(f'  Parameters: {sum(p.numel() for p in test_cbam.parameters()):,}')\n","    print(f'  Device: {\"GPU\" if torch.cuda.is_available() else \"CPU\"}')\n","\n","except Exception as e:\n","    print(f'‚úó CBAM test failed: {e}')\n","    raise\n","\n","print('='*70)"]},{"cell_type":"markdown","id":"50616f69","metadata":{"id":"50616f69"},"source":["## üîó Step 5: Register CBAM with Ultralytics\n","\n","Register the CBAM module so YOLOv8 can use it."]},{"cell_type":"code","execution_count":6,"id":"8e16430b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8e16430b","executionInfo":{"status":"ok","timestamp":1762496741693,"user_tz":-360,"elapsed":18,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"8b28f03d-a6cf-4fbe-c7dd-ccea96ce3f14"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","CBAM REGISTRATION\n","======================================================================\n","‚úì CBAM successfully registered with Ultralytics\n","‚úì YOLOv8 can now use CBAM modules in model architecture\n","======================================================================\n"]}],"source":["from ultralytics.nn import tasks\n","\n","# Register CBAM module\n","tasks.CBAM = CBAM\n","\n","print('='*70)\n","print('CBAM REGISTRATION')\n","print('='*70)\n","\n","# Verify registration\n","if hasattr(tasks, 'CBAM'):\n","    print('‚úì CBAM successfully registered with Ultralytics')\n","    print('‚úì YOLOv8 can now use CBAM modules in model architecture')\n","else:\n","    print('‚úó CBAM registration failed!')\n","    raise RuntimeError('CBAM module registration failed')\n","\n","print('='*70)"]},{"cell_type":"markdown","id":"4086e941","metadata":{"id":"4086e941"},"source":["## üèóÔ∏è Step 6: Create Model Architecture\n","\n","Define YOLOv8n-CBAM architecture in YAML format."]},{"cell_type":"code","execution_count":10,"id":"066d642d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"066d642d","executionInfo":{"status":"ok","timestamp":1762497020895,"user_tz":-360,"elapsed":70,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"67bf0d88-32ed-4521-d985-c7fa6c857131"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","MODEL ARCHITECTURE\n","======================================================================\n","‚úì YOLOv8n-CBAM architecture created\n","‚úì Configuration saved to: /content/yolov8_cbam/yolov8n-cbam.yaml\n","‚úì CBAM modules: 4 (with ACTUAL scaled channels: 32, 64, 128, 256)\n","‚úì Number of classes: 10\n","======================================================================\n","\n","CBAM channel specifications:\n","  Layer 3 CBAM: 32 channels  (matches C2f[128] output after 0.25 scaling)\n","  Layer 6 CBAM: 64 channels  (matches C2f[256] output after 0.25 scaling)\n","  Layer 9 CBAM: 128 channels (matches C2f[512] output after 0.25 scaling)\n","  Layer 12 CBAM: 256 channels (matches C2f[1024] output after 0.25 scaling)\n","======================================================================\n"]}],"source":["import yaml\n","\n","# YOLOv8n-CBAM architecture definition\n","# CRITICAL FIX: YOLOv8 does NOT auto-scale custom module arguments!\n","# We must manually specify the ACTUAL scaled channel counts for CBAM\n","\n","# For 'n' variant: width_multiple = 0.25\n","# So: 128*0.25=32, 256*0.25=64, 512*0.25=128, 1024*0.25=256\n","\n","model_config = {\n","    'nc': 10,  # Number of classes (Bangladesh road vehicles)\n","    'scales': {\n","        # Format: [depth_multiple, width_multiple, max_channels]\n","        # 'n' variant uses 0.33 depth scaling and 0.25 width scaling\n","        'n': [0.33, 0.25, 1024]\n","    },\n","    'backbone': [\n","        # [from, repeats, module, args]\n","        # Standard modules (Conv, C2f) get auto-scaled by YOLOv8\n","        # Custom modules (CBAM) need ACTUAL scaled values!\n","\n","        [-1, 1, 'Conv', [64, 3, 2]],   # 0-P1/2\n","        [-1, 1, 'Conv', [128, 3, 2]],  # 1-P2/4\n","        [-1, 3, 'C2f', [128, True]],   # 2 ‚Üí outputs 32 channels (128*0.25)\n","        [-1, 1, 'CBAM', [32]],         # 3-CBAM ‚Üí 32 channels (ACTUAL scaled value!)\n","\n","        [-1, 1, 'Conv', [256, 3, 2]],  # 4-P3/8\n","        [-1, 6, 'C2f', [256, True]],   # 5 ‚Üí outputs 64 channels (256*0.25)\n","        [-1, 1, 'CBAM', [64]],         # 6-CBAM ‚Üí 64 channels (ACTUAL scaled value!)\n","\n","        [-1, 1, 'Conv', [512, 3, 2]],  # 7-P4/16\n","        [-1, 6, 'C2f', [512, True]],   # 8 ‚Üí outputs 128 channels (512*0.25)\n","        [-1, 1, 'CBAM', [128]],        # 9-CBAM ‚Üí 128 channels (ACTUAL scaled value!)\n","\n","        [-1, 1, 'Conv', [1024, 3, 2]], # 10-P5/32\n","        [-1, 3, 'C2f', [1024, True]],  # 11 ‚Üí outputs 256 channels (1024*0.25=256)\n","        [-1, 1, 'CBAM', [256]],        # 12-CBAM ‚Üí 256 channels (ACTUAL scaled value!)\n","\n","        [-1, 1, 'SPPF', [1024, 5]],    # 13 (SPPF layer)\n","    ],\n","    'head': [\n","        # Detection head with feature pyramid\n","        [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],  # 14\n","        [[-1, 9], 1, 'Concat', [1]],                   # 15 (concat with CBAM layer 9)\n","        [-1, 3, 'C2f', [512]],                         # 16 (P4/16-medium)\n","\n","        [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],  # 17\n","        [[-1, 6], 1, 'Concat', [1]],                   # 18 (concat with CBAM layer 6)\n","        [-1, 3, 'C2f', [256]],                         # 19 (P3/8-small)\n","\n","        [-1, 1, 'Conv', [256, 3, 2]],                  # 20\n","        [[-1, 16], 1, 'Concat', [1]],                  # 21\n","        [-1, 3, 'C2f', [512]],                         # 22 (P4/16-medium)\n","\n","        [-1, 1, 'Conv', [512, 3, 2]],                  # 23\n","        [[-1, 13], 1, 'Concat', [1]],                  # 24\n","        [-1, 3, 'C2f', [1024]],                        # 25 (P5/32-large)\n","\n","        [[19, 22, 25], 1, 'Detect', ['nc']],           # 26 Detect(P3, P4, P5)\n","    ]\n","}\n","\n","# Save model configuration\n","model_yaml_path = f'{COLAB_WORK_DIR}/yolov8n-cbam.yaml'\n","with open(model_yaml_path, 'w') as f:\n","    yaml.dump(model_config, f, default_flow_style=False, sort_keys=False)\n","\n","print('='*70)\n","print('MODEL ARCHITECTURE')\n","print('='*70)\n","print(f'‚úì YOLOv8n-CBAM architecture created')\n","print(f'‚úì Configuration saved to: {model_yaml_path}')\n","print(f'‚úì CBAM modules: 4 (with ACTUAL scaled channels: 32, 64, 128, 256)')\n","print(f'‚úì Number of classes: 10')\n","print('='*70)\n","print('\\nCBAM channel specifications:')\n","print('  Layer 3 CBAM: 32 channels  (matches C2f[128] output after 0.25 scaling)')\n","print('  Layer 6 CBAM: 64 channels  (matches C2f[256] output after 0.25 scaling)')\n","print('  Layer 9 CBAM: 128 channels (matches C2f[512] output after 0.25 scaling)')\n","print('  Layer 12 CBAM: 256 channels (matches C2f[1024] output after 0.25 scaling)')\n","print('='*70)"]},{"cell_type":"markdown","id":"b3f40194","metadata":{"id":"b3f40194"},"source":["## üîç Step 7: Validate Dataset Configuration\n","\n","Check if dataset and configuration are correct before training."]},{"cell_type":"code","execution_count":11,"id":"63c2b5cd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63c2b5cd","executionInfo":{"status":"ok","timestamp":1762497023201,"user_tz":-360,"elapsed":51,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"88e7a6d2-99d6-4b8c-cef4-2b14f2344c48"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","DATASET VALIDATION\n","======================================================================\n","‚úì Loaded data.yaml from: /content/drive/MyDrive/YOLOv8 Traffic/data.yaml\n","‚úì Field \"path\" present\n","‚úì Field \"train\" present\n","‚úì Field \"val\" present\n","‚úì Field \"nc\" present\n","‚úì Field \"names\" present\n","‚úì Number of classes matches: 10\n","\n","Classes (10):\n","  0: bicycle\n","  1: bus\n","  2: car\n","  3: cng\n","  4: auto\n","  5: bike\n","  6: Multi-Class\n","  7: rickshaw\n","  8: truck\n","  9: van\n","\n","Dataset path: /content/drive/MyDrive/YOLOv8 Traffic/dataset\n","‚úì Dataset path exists\n","‚úì Training images path exists: 868 images found\n","‚úì Validation images path exists: 217 images found\n","======================================================================\n"]}],"source":["import yaml\n","from pathlib import Path\n","\n","print('='*70)\n","print('DATASET VALIDATION')\n","print('='*70)\n","\n","# Load data.yaml\n","try:\n","    with open(DATA_YAML, 'r') as f:\n","        data_config = yaml.safe_load(f)\n","    print(f'‚úì Loaded data.yaml from: {DATA_YAML}')\n","except Exception as e:\n","    print(f'‚úó Failed to load data.yaml: {e}')\n","    raise\n","\n","# Validate required fields\n","required_fields = ['path', 'train', 'val', 'nc', 'names']\n","missing_fields = []\n","\n","for field in required_fields:\n","    if field in data_config:\n","        print(f'‚úì Field \"{field}\" present')\n","    else:\n","        print(f'‚úó Missing required field: \"{field}\"')\n","        missing_fields.append(field)\n","\n","if missing_fields:\n","    raise ValueError(f'Missing required fields in data.yaml: {missing_fields}')\n","\n","# Validate number of classes\n","nc = data_config['nc']\n","names = data_config['names']\n","num_names = len(names) if isinstance(names, (list, dict)) else 0\n","\n","if nc == num_names:\n","    print(f'‚úì Number of classes matches: {nc}')\n","else:\n","    print(f'‚úó Class mismatch: nc={nc} but found {num_names} names')\n","    raise ValueError(f'Number of classes mismatch')\n","\n","# Display class names\n","print(f'\\nClasses ({nc}):')\n","if isinstance(names, dict):\n","    for idx, name in names.items():\n","        print(f'  {idx}: {name}')\n","elif isinstance(names, list):\n","    for idx, name in enumerate(names):\n","        print(f'  {idx}: {name}')\n","\n","# Check dataset path\n","dataset_path = data_config['path']\n","print(f'\\nDataset path: {dataset_path}')\n","\n","if os.path.exists(dataset_path):\n","    print(f'‚úì Dataset path exists')\n","\n","    # Check train/val directories\n","    train_images = Path(dataset_path) / data_config['train']\n","    val_images = Path(dataset_path) / data_config['val']\n","\n","    if train_images.exists():\n","        train_count = len(list(train_images.glob('*.jpg'))) + len(list(train_images.glob('*.png')))\n","        print(f'‚úì Training images path exists: {train_count} images found')\n","    else:\n","        print(f'‚ö† Training images path not found: {train_images}')\n","\n","    if val_images.exists():\n","        val_count = len(list(val_images.glob('*.jpg'))) + len(list(val_images.glob('*.png')))\n","        print(f'‚úì Validation images path exists: {val_count} images found')\n","    else:\n","        print(f'‚ö† Validation images path not found: {val_images}')\n","else:\n","    print(f'‚ö† Dataset path not accessible from Colab')\n","    print(f'  This may be OK if the path is relative and will be resolved during training')\n","\n","print('='*70)"]},{"cell_type":"markdown","id":"6c7e4157","metadata":{"id":"6c7e4157"},"source":["## üèãÔ∏è Step 8: Load and Validate Model\n","\n","Load the YOLOv8n-CBAM model and pretrained weights."]},{"cell_type":"code","execution_count":13,"id":"f94e9f4d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f94e9f4d","executionInfo":{"status":"ok","timestamp":1762497173865,"user_tz":-360,"elapsed":1698,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"148d87da-5f9d-49de-9c33-d30e0e010651"},"outputs":[{"output_type":"stream","name":"stderr","text":["\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1       226  CBAM                                         [32]                          \n","  4                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  5                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  6                  -1  1       610  CBAM                                         [64]                          \n","  7                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  8                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  9                  -1  1      2146  CBAM                                         [128]                         \n"," 10                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n"," 11                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n"," 12                  -1  1      8290  CBAM                                         [256]                         \n"," 13                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 15             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 16                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 18             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 19                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n"," 20                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 21            [-1, 16]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 22                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n"," 23                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 24            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 25                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 26        [19, 22, 25]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n"]},{"output_type":"stream","name":"stdout","text":["======================================================================\n","MODEL LOADING\n","======================================================================\n","Loading model from: /content/yolov8_cbam/yolov8n-cbam.yaml\n","  CBAM initialized: c1=32, reduction_ratio=16, shortcut=True\n","  CBAM initialized: c1=64, reduction_ratio=16, shortcut=True\n","  CBAM initialized: c1=128, reduction_ratio=16, shortcut=True\n","  CBAM initialized: c1=256, reduction_ratio=16, shortcut=True\n"]},{"output_type":"stream","name":"stderr","text":["YOLOv8n-cbam summary: 261 layers, 3024070 parameters, 3024054 gradients, 8.2 GFLOPs\n","\n"]},{"output_type":"stream","name":"stdout","text":["‚úì Model architecture loaded\n","\n","Loading pretrained weights: yolov8n.pt\n","  Retrying with adjusted torch.load settings...\n"]},{"output_type":"stream","name":"stderr","text":["Transferred 41/367 items from pretrained weights\n"]},{"output_type":"stream","name":"stdout","text":["‚úì Pretrained weights loaded (transfer learning enabled)\n","\n","======================================================================\n","MODEL INFORMATION\n","======================================================================\n","\n","======================================================================\n","FORWARD PASS TEST\n","======================================================================\n","‚úì Forward pass successful\n","  Input shape: (1, 3, 640, 640)\n","  Device: cuda:0\n","  Memory allocated: 19.3 MB\n","\n","‚úì Model ready for training!\n","======================================================================\n"]}],"source":["import torch.serialization\n","\n","# Add safe globals for PyTorch 2.6 compatibility\n","torch.serialization.add_safe_globals(['ultralytics.nn.tasks.DetectionModel'])\n","\n","print('='*70)\n","print('MODEL LOADING')\n","print('='*70)\n","\n","try:\n","    # Load model architecture\n","    print(f'Loading model from: {model_yaml_path}')\n","    model = YOLO(model_yaml_path)\n","    print('‚úì Model architecture loaded')\n","\n","    # Load pretrained weights (with PyTorch 2.6 compatibility)\n","    print(f'\\nLoading pretrained weights: {PRETRAINED_WEIGHTS}')\n","    try:\n","        model = model.load(PRETRAINED_WEIGHTS)\n","    except Exception as weight_error:\n","        # If weights_only causes issues, try with explicit setting\n","        print('  Retrying with adjusted torch.load settings...')\n","        import torch\n","        original_weights_only = getattr(torch.serialization, '_use_new_zipfile_serialization', None)\n","        try:\n","            # Temporarily allow non-weights-only loading for trusted ultralytics weights\n","            model.ckpt = None\n","            from ultralytics.nn.tasks import attempt_load_one_weight\n","            # Patch torch.load temporarily\n","            _original_load = torch.load\n","            def _patched_load(f, map_location=None, *args, **kwargs):\n","                kwargs['weights_only'] = False\n","                return _original_load(f, map_location=map_location, *args, **kwargs)\n","            torch.load = _patched_load\n","            model = model.load(PRETRAINED_WEIGHTS)\n","            torch.load = _original_load\n","        except Exception as e:\n","            torch.load = _original_load  # Restore original\n","            raise e\n","\n","    print('‚úì Pretrained weights loaded (transfer learning enabled)')\n","\n","    # Display model info\n","    print('\\n' + '='*70)\n","    print('MODEL INFORMATION')\n","    print('='*70)\n","    model.info(verbose=False)\n","\n","    # Test forward pass\n","    print('\\n' + '='*70)\n","    print('FORWARD PASS TEST')\n","    print('='*70)\n","\n","    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","    model.model.to(device)\n","    model.model.eval()\n","\n","    # Test with 640x640 input\n","    test_input = torch.randn(1, 3, 640, 640).to(device)\n","\n","    with torch.no_grad():\n","        test_output = model.model(test_input)\n","\n","    print(f'‚úì Forward pass successful')\n","    print(f'  Input shape: {tuple(test_input.shape)}')\n","    print(f'  Device: {device}')\n","    print(f'  Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB' if torch.cuda.is_available() else '  Device: CPU')\n","\n","    # Set back to training mode\n","    model.model.train()\n","\n","    print('\\n‚úì Model ready for training!')\n","    print('='*70)\n","\n","except Exception as e:\n","    print(f'\\n‚úó Model loading failed: {e}')\n","    import traceback\n","    print('\\nFull error traceback:')\n","    traceback.print_exc()\n","    raise"]},{"cell_type":"markdown","id":"33cc3e05","metadata":{"id":"33cc3e05"},"source":["## ‚öôÔ∏è Step 9: Configure Training Parameters\n","\n","Set up hyperparameters optimized for Google Colab T4 GPU."]},{"cell_type":"code","execution_count":16,"id":"e3506016","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3506016","executionInfo":{"status":"ok","timestamp":1762497491002,"user_tz":-360,"elapsed":63,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"d3660e46-4559-49b0-8700-7d694b74bd6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","TRAINING CONFIGURATION (Optimized for T4 GPU)\n","======================================================================\n","Epochs: 100\n","Batch size: 16\n","Image size: 640\n","Device: GPU 0 (T4)\n","Mixed Precision: True\n","Learning rate: 0.01 ‚Üí 0.0001\n","Early stopping patience: 50 epochs\n","Project: yolov8_cbam_training / bd_vehicles_run\n","Results will be saved to: runs/detect/yolov8_cbam_training/bd_vehicles_run\n","======================================================================\n","\n","GPU Memory: 14.7 GB\n","‚ö† WARNING: Less than 15GB GPU memory detected\n","  Consider reducing batch size if you encounter OOM errors\n","\n","‚úì Training configuration ready!\n","‚úì Weights & Biases (wandb) disabled for simpler setup\n"]}],"source":["# Training configuration optimized for T4 GPU\n","TRAINING_CONFIG = {\n","    'data': DATA_YAML,\n","    'epochs': 100,                    # Number of epochs\n","    'imgsz': 640,                     # Image size\n","    'batch': 16,                      # Batch size (optimal for T4 16GB)\n","    'device': 0,                      # GPU device\n","    'workers': 2,                     # DataLoader workers (Colab has limited CPU)\n","    'project': 'yolov8_cbam_training',  # Project name (NOT a path - wandb compatible)\n","    'name': 'bd_vehicles_run',          # Experiment name\n","    'exist_ok': True,                 # Overwrite existing project\n","    'pretrained': True,               # Use pretrained weights\n","    'optimizer': 'auto',              # SGD for small batches, AdamW for large\n","    'verbose': True,                  # Verbose output\n","    'seed': 42,                       # Random seed for reproducibility\n","    'deterministic': False,           # Faster training (non-deterministic)\n","    'single_cls': False,              # Multi-class detection\n","    'rect': False,                    # Rectangular training\n","    'cos_lr': True,                   # Cosine learning rate scheduler\n","    'close_mosaic': 10,               # Disable mosaic last N epochs\n","    'resume': False,                  # Resume from last checkpoint\n","    'amp': True,                      # Automatic Mixed Precision (faster on T4)\n","    'fraction': 1.0,                  # Use full dataset\n","    'profile': False,                 # Don't profile (saves memory)\n","    'freeze': None,                   # Don't freeze layers\n","    'save': True,                     # Save checkpoints\n","    'save_period': -1,                # Save checkpoint every N epochs (-1 = last only)\n","    'cache': False,                   # Don't cache images (saves RAM)\n","    'patience': 50,                   # Early stopping patience\n","    'plots': True,                    # Generate plots\n","    'val': True,                      # Validate during training\n","\n","    # Hyperparameters\n","    'lr0': 0.01,                      # Initial learning rate\n","    'lrf': 0.01,                      # Final learning rate (lr0 * lrf)\n","    'momentum': 0.937,                # SGD momentum\n","    'weight_decay': 0.0005,           # Weight decay\n","    'warmup_epochs': 3.0,             # Warmup epochs\n","    'warmup_momentum': 0.8,           # Warmup momentum\n","    'warmup_bias_lr': 0.1,            # Warmup bias learning rate\n","    'box': 7.5,                       # Box loss gain\n","    'cls': 0.5,                       # Class loss gain\n","    'dfl': 1.5,                       # DFL loss gain\n","    'label_smoothing': 0.0,           # Label smoothing\n","    'nbs': 64,                        # Nominal batch size\n","    'hsv_h': 0.015,                   # HSV-Hue augmentation\n","    'hsv_s': 0.7,                     # HSV-Saturation augmentation\n","    'hsv_v': 0.4,                     # HSV-Value augmentation\n","    'degrees': 0.0,                   # Rotation augmentation\n","    'translate': 0.1,                 # Translation augmentation\n","    'scale': 0.5,                     # Scale augmentation\n","    'shear': 0.0,                     # Shear augmentation\n","    'perspective': 0.0,               # Perspective augmentation\n","    'flipud': 0.0,                    # Vertical flip probability\n","    'fliplr': 0.5,                    # Horizontal flip probability\n","    'mosaic': 1.0,                    # Mosaic augmentation probability\n","    'mixup': 0.0,                     # Mixup augmentation probability\n","    'copy_paste': 0.0,                # Copy-paste augmentation probability\n","}\n","\n","# Disable wandb if it's causing issues (optional - uncomment if needed)\n","import os\n","os.environ['WANDB_DISABLED'] = 'true'\n","\n","# Display configuration\n","print('='*70)\n","print('TRAINING CONFIGURATION (Optimized for T4 GPU)')\n","print('='*70)\n","print(f'Epochs: {TRAINING_CONFIG[\"epochs\"]}')\n","print(f'Batch size: {TRAINING_CONFIG[\"batch\"]}')\n","print(f'Image size: {TRAINING_CONFIG[\"imgsz\"]}')\n","print(f'Device: GPU {TRAINING_CONFIG[\"device\"]} (T4)' if torch.cuda.is_available() else 'CPU')\n","print(f'Mixed Precision: {TRAINING_CONFIG[\"amp\"]}')\n","print(f'Learning rate: {TRAINING_CONFIG[\"lr0\"]} ‚Üí {TRAINING_CONFIG[\"lr0\"] * TRAINING_CONFIG[\"lrf\"]}')\n","print(f'Early stopping patience: {TRAINING_CONFIG[\"patience\"]} epochs')\n","print(f'Project: {TRAINING_CONFIG[\"project\"]} / {TRAINING_CONFIG[\"name\"]}')\n","print(f'Results will be saved to: runs/detect/{TRAINING_CONFIG[\"project\"]}/{TRAINING_CONFIG[\"name\"]}')\n","print('='*70)\n","\n","# Memory check for T4\n","if torch.cuda.is_available():\n","    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n","    print(f'\\nGPU Memory: {total_memory:.1f} GB')\n","\n","    if total_memory < 15:\n","        print('‚ö† WARNING: Less than 15GB GPU memory detected')\n","        print('  Consider reducing batch size if you encounter OOM errors')\n","    else:\n","        print('‚úì GPU memory sufficient for batch size 16')\n","\n","print('\\n‚úì Training configuration ready!')\n","print('‚úì Weights & Biases (wandb) disabled for simpler setup')\n"]},{"cell_type":"markdown","id":"ba35e636","metadata":{"id":"ba35e636"},"source":["## üöÄ Step 10: Start Training\n","\n","**This will take approximately 3-4 hours on a T4 GPU for 100 epochs.**\n","\n","‚ö†Ô∏è **Important**: Make sure your Colab session stays active. Consider:\n","- Using Colab Pro for longer sessions\n","- Periodically checking the training progress\n","- The training will save checkpoints automatically"]},{"cell_type":"code","execution_count":17,"id":"55473eea","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"55473eea","executionInfo":{"status":"error","timestamp":1762513239116,"user_tz":-360,"elapsed":15746101,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"523ff1ba-75a6-4f30-9221-261a41c4058e"},"outputs":[{"output_type":"stream","name":"stderr","text":["New https://pypi.org/project/ultralytics/8.3.225 available üòÉ Update with 'pip install -U ultralytics'\n","Ultralytics YOLOv8.0.200 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/content/yolov8_cbam/yolov8n-cbam.yaml, data=/content/drive/MyDrive/YOLOv8 Traffic/data.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=2, project=yolov8_cbam_training, name=bd_vehicles_run, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=42, deterministic=False, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=yolov8_cbam_training/bd_vehicles_run\n"]},{"output_type":"stream","name":"stdout","text":["======================================================================\n","STARTING TRAINING\n","======================================================================\n","Start time: 2025-11-07 06:38:12\n","Expected duration: ~3-4 hours (100 epochs on T4 GPU)\n","======================================================================\n","\n","‚è≥ Training in progress... Please wait.\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1       226  CBAM                                         [32]                          \n","  4                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  5                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  6                  -1  1       610  CBAM                                         [64]                          \n","  7                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  8                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  9                  -1  1      2146  CBAM                                         [128]                         \n"," 10                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n"," 11                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n"," 12                  -1  1      8290  CBAM                                         [256]                         \n"," 13                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 15             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 16                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 18             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 19                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n"," 20                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 21            [-1, 16]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 22                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n"," 23                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 24            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 25                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 26        [19, 22, 25]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n"]},{"output_type":"stream","name":"stdout","text":["  CBAM initialized: c1=32, reduction_ratio=16, shortcut=True\n","  CBAM initialized: c1=64, reduction_ratio=16, shortcut=True\n","  CBAM initialized: c1=128, reduction_ratio=16, shortcut=True\n","  CBAM initialized: c1=256, reduction_ratio=16, shortcut=True\n"]},{"output_type":"stream","name":"stderr","text":["YOLOv8n-cbam summary: 261 layers, 3024070 parameters, 3024054 gradients, 8.2 GFLOPs\n","\n","Transferred 367/367 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov8_cbam_training/bd_vehicles_run', view at http://localhost:6006/\n","/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n","  | |_| | '_ \\/ _` / _` |  _/ -_)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"]},{"name":"stdout","output_type":"stream","text":[" ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhandaker15-5383\u001b[0m (\u001b[33mkhandaker15-5383-daffodil-international-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.22.3"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251107_063948-rzwxy8gc</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/khandaker15-5383-daffodil-international-university/yolov8_cbam_training/runs/rzwxy8gc' target=\"_blank\">bd_vehicles_run</a></strong> to <a href='https://wandb.ai/khandaker15-5383-daffodil-international-university/yolov8_cbam_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/khandaker15-5383-daffodil-international-university/yolov8_cbam_training' target=\"_blank\">https://wandb.ai/khandaker15-5383-daffodil-international-university/yolov8_cbam_training</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/khandaker15-5383-daffodil-international-university/yolov8_cbam_training/runs/rzwxy8gc' target=\"_blank\">https://wandb.ai/khandaker15-5383-daffodil-international-university/yolov8_cbam_training/runs/rzwxy8gc</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Freezing layer 'model.26.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks skipped ‚ö†Ô∏è. Unable to load YOLOv8n due to possible Ultralytics package modifications. Setting 'amp=True'. If you experience zero-mAP or NaN losses you can disable AMP with amp=False.\n","/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py:239: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = amp.GradScaler(enabled=self.amp)\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1CA2eiw-QqvOG-A8SZwgNxgr29ai5UAcR/YOLOv8 Traffic/dataset/labels/train.cache... 868 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 868/868 [00:00<?, ?it/s]\n","/usr/local/lib/python3.12/dist-packages/ultralytics/data/augment.py:805: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n","  A.ImageCompression(quality_lower=75, p=0.0)]  # transforms\n","/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n","  self._set_keys()\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1CA2eiw-QqvOG-A8SZwgNxgr29ai5UAcR/YOLOv8 Traffic/dataset/labels/val.cache... 217 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:00<?, ?it/s]\n","Plotting labels to yolov8_cbam_training/bd_vehicles_run/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 76 weight(decay=0.0005), 63 bias(decay=0.0)\n","/tmp/ipython-input-1985054845.py:176: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if actual_channels != self.c1:\n","/tmp/ipython-input-1985054845.py:185: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if torch.isnan(x).any():\n","/tmp/ipython-input-1985054845.py:187: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if torch.isinf(x).any():\n","/tmp/ipython-input-1985054845.py:204: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if torch.isnan(x).any():\n","/tmp/ipython-input-1985054845.py:206: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if torch.isinf(x).any():\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1myolov8_cbam_training/bd_vehicles_run\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","      1/100      2.32G      3.104      5.411      4.261         14        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [09:13<00:00, 10.06s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:37<00:00,  5.31s/it]\n","                   all        217        254   0.000204     0.0364   0.000132   3.95e-05\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","      2/100      2.35G      2.862      4.761      3.789         11        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:07<00:00,  2.33s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.53s/it]\n","                   all        217        254     0.0489      0.149     0.0354    0.00979\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","      3/100      2.36G      2.613      4.149      3.332          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:03<00:00,  2.24s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  3.67s/it]\n","                   all        217        254     0.0115      0.201     0.0129    0.00282\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","      4/100      2.38G      2.264      3.701      2.886         14        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:15<00:00,  2.46s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.18s/it]\n","                   all        217        254      0.201      0.123     0.0576     0.0254\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","      5/100      2.39G      2.023      3.344      2.667         11        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:07<00:00,  2.31s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  3.66s/it]\n","                   all        217        254      0.322       0.32      0.228      0.119\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","      6/100      2.36G      1.868      3.053      2.486          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:06<00:00,  2.30s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.39s/it]\n","                   all        217        254      0.358      0.282      0.223      0.107\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","      7/100      2.35G      1.732      2.832      2.372          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:57<00:00,  2.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.52s/it]\n","                   all        217        254       0.27      0.527      0.305      0.191\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","      8/100      2.37G      1.637       2.65      2.277          7        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:10<00:00,  2.37s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.39s/it]\n","                   all        217        254      0.532      0.438      0.404       0.25\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","      9/100      2.38G      1.532      2.526      2.175          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:09<00:00,  2.35s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  3.59s/it]\n","                   all        217        254      0.397      0.464      0.419      0.273\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     10/100      2.39G      1.497      2.399      2.098          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:11<00:00,  2.40s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.15s/it]\n","                   all        217        254      0.389      0.642      0.477      0.311\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     11/100      2.36G      1.442      2.245      2.026          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:09<00:00,  2.35s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.16s/it]\n","                   all        217        254      0.507      0.463      0.489      0.331\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     12/100      2.36G      1.423      2.166      2.003          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:05<00:00,  2.29s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.33s/it]\n","                   all        217        254      0.524      0.518      0.539      0.384\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     13/100      2.37G      1.342      2.009      1.939          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:04<00:00,  2.25s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.27s/it]\n","                   all        217        254      0.683      0.504      0.607      0.441\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     14/100      2.38G      1.331      2.018      1.928         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:08<00:00,  2.34s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  3.71s/it]\n","                   all        217        254       0.55      0.611      0.592      0.433\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     15/100      2.37G      1.311      1.887      1.864          7        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:10<00:00,  2.37s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  3.60s/it]\n","                   all        217        254      0.521      0.601       0.61      0.425\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     16/100      2.36G      1.271       1.85      1.842          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:12<00:00,  2.42s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  3.60s/it]\n","                   all        217        254      0.594      0.675      0.658      0.447\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     17/100      2.37G      1.239      1.741      1.815          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:08<00:00,  2.33s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.52s/it]\n","                   all        217        254      0.624      0.642       0.73      0.558\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     18/100      2.38G      1.209       1.68      1.778          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:09<00:00,  2.35s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.54s/it]\n","                   all        217        254      0.659      0.704      0.723        0.5\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     19/100      2.39G      1.202      1.644       1.76         11        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:12<00:00,  2.41s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.20s/it]\n","                   all        217        254      0.716      0.668      0.783      0.573\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     20/100      2.36G      1.182      1.609      1.741         11        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:09<00:00,  2.36s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.37s/it]\n","                   all        217        254      0.716      0.689      0.777      0.572\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     21/100      2.36G      1.158      1.549      1.707          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:02<00:00,  2.22s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.28s/it]\n","                   all        217        254      0.681      0.682      0.755      0.569\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     22/100      2.37G      1.132      1.517      1.675         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:09<00:00,  2.35s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.18s/it]\n","                   all        217        254      0.531      0.682      0.677      0.521\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     23/100      2.37G      1.149       1.51      1.683         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:12<00:00,  2.41s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:26<00:00,  3.74s/it]\n","                   all        217        254      0.717      0.724      0.804      0.622\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     24/100      2.39G      1.071       1.42      1.634          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:04<00:00,  2.27s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.48s/it]\n","                   all        217        254      0.698       0.71      0.768      0.588\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     25/100      2.41G      1.083      1.385      1.633         11        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:15<00:00,  2.47s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.43s/it]\n","                   all        217        254      0.651      0.687      0.755       0.56\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     26/100      2.36G      1.086      1.363      1.638          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:13<00:00,  2.42s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.21s/it]\n","                   all        217        254      0.714      0.764      0.833      0.658\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     27/100      2.34G      1.068      1.389      1.611          7        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:16<00:00,  2.49s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.20s/it]\n","                   all        217        254      0.699      0.717      0.798      0.626\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     28/100      2.36G      1.057      1.394      1.608         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:15<00:00,  2.46s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  3.64s/it]\n","                   all        217        254      0.754      0.768      0.841      0.668\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     29/100      2.37G      1.032       1.33      1.588          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:09<00:00,  2.36s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.47s/it]\n","                   all        217        254      0.729      0.763      0.826      0.647\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     30/100      2.38G      1.036      1.315      1.581         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:13<00:00,  2.42s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.17s/it]\n","                   all        217        254      0.792      0.715      0.822      0.643\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     31/100      2.38G      1.025      1.275      1.559         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:11<00:00,  2.40s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.21s/it]\n","                   all        217        254      0.787      0.701      0.827      0.639\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     32/100      2.36G      0.974       1.23      1.528         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:12<00:00,  2.42s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  3.65s/it]\n","                   all        217        254      0.779      0.805      0.858      0.681\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     33/100      2.36G      1.033       1.28      1.571          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:06<00:00,  2.31s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.41s/it]\n","                   all        217        254      0.807      0.747      0.873      0.693\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     34/100      2.37G      0.958      1.178      1.518          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:10<00:00,  2.38s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.25s/it]\n","                   all        217        254      0.827       0.79      0.877      0.683\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     35/100      2.37G     0.9787      1.179      1.521          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:10<00:00,  2.37s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.17s/it]\n","                   all        217        254      0.761      0.776       0.87      0.698\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     36/100      2.39G     0.9866      1.225       1.53          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:12<00:00,  2.41s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  3.64s/it]\n","                   all        217        254        0.8      0.837      0.891       0.71\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     37/100      2.36G     0.9501      1.166      1.496          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:09<00:00,  2.36s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.26s/it]\n","                   all        217        254      0.845      0.761      0.879      0.683\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     38/100      2.36G     0.9719      1.167      1.499         13        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:10<00:00,  2.37s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.23s/it]\n","                   all        217        254        0.8      0.844      0.884       0.68\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     39/100      2.36G     0.9386      1.134      1.473          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:10<00:00,  2.37s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.50s/it]\n","                   all        217        254      0.835      0.785      0.892      0.709\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     40/100      2.38G     0.9349      1.101      1.473          7        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:07<00:00,  2.33s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.53s/it]\n","                   all        217        254      0.793      0.847      0.888      0.716\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     41/100      2.39G     0.9313      1.115      1.474         14        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:06<00:00,  2.31s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.34s/it]\n","                   all        217        254      0.852      0.761      0.884      0.712\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     42/100      2.41G     0.9182      1.098      1.459          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:12<00:00,  2.41s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.20s/it]\n","                   all        217        254      0.845      0.859      0.906      0.725\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     43/100      2.36G     0.9231      1.096       1.46         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:56<00:00,  2.11s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.50s/it]\n","                   all        217        254      0.746      0.802      0.843      0.678\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     44/100      2.36G     0.8996      1.071      1.443         11        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:10<00:00,  2.37s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.34s/it]\n","                   all        217        254      0.816      0.831      0.908      0.738\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     45/100      2.37G     0.9026      1.039       1.44          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:10<00:00,  2.38s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  3.63s/it]\n","                   all        217        254      0.778      0.866      0.883      0.713\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     46/100      2.38G     0.8967      1.018      1.431          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:10<00:00,  2.36s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.11s/it]\n","                   all        217        254      0.853      0.838      0.909       0.73\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     47/100      2.39G     0.8836      1.027      1.427         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:58<00:00,  2.16s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.37s/it]\n","                   all        217        254      0.879       0.81      0.911      0.743\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     48/100      2.36G     0.8933      1.028      1.429          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:55<00:00,  2.11s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.27s/it]\n","                   all        217        254      0.851      0.827        0.9      0.731\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     49/100      2.36G     0.8767      1.007       1.41          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:01<00:00,  2.21s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  2.93s/it]\n","                   all        217        254      0.848      0.852      0.916      0.739\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     50/100      2.37G     0.8649          1        1.4         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:05<00:00,  2.28s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.32s/it]\n","                   all        217        254      0.833      0.844      0.923      0.749\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     51/100      2.37G     0.8531     0.9634      1.407          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:04<00:00,  2.27s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  2.96s/it]\n","                   all        217        254      0.826      0.858      0.918      0.755\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     52/100      2.39G     0.8384       0.96       1.38         15        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:01<00:00,  2.20s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.27s/it]\n","                   all        217        254      0.844      0.839      0.915      0.735\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     53/100      2.41G       0.86     0.9773      1.415         13        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:58<00:00,  2.15s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.43s/it]\n","                   all        217        254      0.834      0.862      0.925      0.755\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     54/100      2.36G     0.8372     0.9701      1.374          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:06<00:00,  2.30s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  2.90s/it]\n","                   all        217        254      0.851      0.857      0.918      0.748\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     55/100      2.36G     0.8114     0.9365      1.353         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:06<00:00,  2.30s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.20s/it]\n","                   all        217        254      0.828      0.853      0.916      0.758\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     56/100      2.37G     0.8254     0.9109      1.369          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:02<00:00,  2.23s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.10s/it]\n","                   all        217        254      0.857      0.882      0.936      0.773\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     57/100      2.38G     0.8137     0.9088      1.349         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:01<00:00,  2.21s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.29s/it]\n","                   all        217        254      0.845      0.886      0.932      0.759\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     58/100      2.39G     0.8118     0.8965      1.362          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:01<00:00,  2.22s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.15s/it]\n","                   all        217        254      0.888      0.885      0.941      0.775\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     59/100      2.36G     0.7985     0.8703      1.338         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:02<00:00,  2.23s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.47s/it]\n","                   all        217        254      0.851      0.912      0.935      0.769\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     60/100      2.36G     0.7892     0.8917      1.353          7        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:03<00:00,  2.25s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.47s/it]\n","                   all        217        254      0.878      0.876       0.94      0.784\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     61/100      2.37G     0.7859      0.858      1.323          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:03<00:00,  2.24s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.30s/it]\n","                   all        217        254      0.916       0.84      0.937      0.776\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     62/100      2.38G     0.7842     0.8843      1.331          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:06<00:00,  2.31s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.44s/it]\n","                   all        217        254      0.887      0.905      0.954      0.794\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     63/100      2.38G     0.8117     0.8867      1.366          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:05<00:00,  2.28s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.04s/it]\n","                   all        217        254      0.867      0.906      0.942      0.779\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     64/100      2.36G     0.7864     0.8531      1.324          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:08<00:00,  2.34s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.05s/it]\n","                   all        217        254      0.884      0.851      0.934      0.777\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     65/100      2.36G     0.7329     0.7934      1.287         11        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:56<00:00,  2.13s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.16s/it]\n","                   all        217        254      0.883      0.878      0.932      0.779\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     66/100      2.37G     0.7775     0.8427      1.314          7        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:01<00:00,  2.21s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.10s/it]\n","                   all        217        254      0.899      0.894      0.948      0.799\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     67/100      2.37G     0.7643     0.8225      1.314         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:05<00:00,  2.27s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.20s/it]\n","                   all        217        254      0.927      0.887      0.951      0.787\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     68/100      2.39G     0.7606     0.7946        1.3          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:01<00:00,  2.20s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.25s/it]\n","                   all        217        254      0.891      0.886      0.951      0.787\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     69/100      2.41G     0.7672     0.8145      1.299         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:02<00:00,  2.22s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.30s/it]\n","                   all        217        254      0.907      0.861      0.942      0.786\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     70/100      2.42G     0.7702     0.7998      1.319          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:08<00:00,  2.33s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.28s/it]\n","                   all        217        254      0.915      0.856      0.942       0.79\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     71/100      2.36G     0.7538     0.7929        1.3         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:59<00:00,  2.18s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.20s/it]\n","                   all        217        254      0.865      0.926       0.95      0.804\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     72/100      2.36G     0.7566     0.7835      1.306          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:02<00:00,  2.23s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.37s/it]\n","                   all        217        254      0.915      0.894      0.953      0.787\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     73/100      2.37G     0.7448     0.7756       1.29          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:00<00:00,  2.19s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.07s/it]\n","                   all        217        254       0.88      0.912      0.951      0.794\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     74/100      2.38G     0.7157     0.7428      1.274         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:01<00:00,  2.21s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.20s/it]\n","                   all        217        254      0.923      0.883       0.95      0.788\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     75/100      2.38G     0.7459     0.7694      1.287          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:59<00:00,  2.17s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.22s/it]\n","                   all        217        254        0.9      0.897      0.955      0.798\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     76/100      2.41G     0.7443     0.7816      1.293          7        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:02<00:00,  2.22s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.46s/it]\n","                   all        217        254      0.898      0.919      0.955      0.802\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     77/100      2.36G     0.7334     0.7543      1.283          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:01<00:00,  2.21s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:19<00:00,  2.83s/it]\n","                   all        217        254      0.902      0.888      0.955      0.804\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     78/100      2.36G     0.7141     0.7676      1.263         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:04<00:00,  2.26s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.26s/it]\n","                   all        217        254      0.907       0.92      0.955      0.802\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     79/100      2.37G     0.7323     0.7841      1.277          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:03<00:00,  2.24s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.13s/it]\n","                   all        217        254      0.918      0.881      0.953      0.807\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     80/100      2.38G     0.7069     0.7199      1.257          7        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:04<00:00,  2.26s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.15s/it]\n","                   all        217        254      0.867      0.946      0.954      0.803\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     81/100      2.39G      0.703     0.7347      1.245         10        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:04<00:00,  2.27s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.12s/it]\n","                   all        217        254      0.898      0.913      0.957      0.811\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     82/100      2.37G     0.7027      0.721      1.259         11        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:59<00:00,  2.17s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.13s/it]\n","                   all        217        254      0.927      0.919      0.962      0.813\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     83/100      2.36G     0.7106     0.7218      1.256         12        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:02<00:00,  2.22s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.37s/it]\n","                   all        217        254      0.915      0.915      0.959      0.808\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     84/100      2.37G     0.7083     0.7266      1.258          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:07<00:00,  2.32s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:25<00:00,  3.69s/it]\n","                   all        217        254       0.92      0.892      0.955      0.814\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     85/100      2.38G     0.7033     0.7339      1.265          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:54<00:00,  2.08s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.05s/it]\n","                   all        217        254      0.919      0.918      0.959      0.813\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     86/100      2.39G     0.6893     0.7162      1.251         11        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:54<00:00,  2.09s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.10s/it]\n","                   all        217        254        0.9      0.921      0.958      0.809\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     87/100       2.4G     0.7053     0.7254      1.261          9        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:05<00:00,  2.29s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  2.96s/it]\n","                   all        217        254      0.922      0.911      0.962      0.815\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     88/100      2.36G     0.7027     0.7206      1.244          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:53<00:00,  2.06s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.42s/it]\n","                   all        217        254      0.905      0.918      0.961      0.812\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     89/100      2.36G     0.6957     0.6994       1.25         11        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:56<00:00,  2.11s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  2.89s/it]\n","                   all        217        254       0.93      0.901      0.963      0.815\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     90/100      2.37G     0.6804     0.7053      1.242          8        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:57<00:00,  2.14s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.33s/it]\n","                   all        217        254      0.931      0.898      0.962      0.819\n","Closing dataloader mosaic\n","/usr/local/lib/python3.12/dist-packages/ultralytics/data/augment.py:805: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n","  A.ImageCompression(quality_lower=75, p=0.0)]  # transforms\n","/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n","  self._set_keys()\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     91/100      2.37G     0.5591     0.5762      1.143          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:18<00:00,  2.52s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.31s/it]\n","                   all        217        254      0.895      0.932      0.961      0.815\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     92/100      2.39G     0.5502     0.5501      1.147          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:02<00:00,  2.23s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.11s/it]\n","                   all        217        254      0.932       0.91      0.961      0.815\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     93/100      2.35G     0.5516     0.5267      1.134          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:00<00:00,  2.19s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.04s/it]\n","                   all        217        254      0.932       0.91      0.962      0.818\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     94/100      2.36G     0.5377     0.5127      1.121          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:08<00:00,  2.34s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  2.98s/it]\n","                   all        217        254      0.929      0.912      0.962       0.82\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     95/100      2.36G     0.5272     0.5058      1.104          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:06<00:00,  2.31s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:22<00:00,  3.18s/it]\n","                   all        217        254      0.909      0.941      0.961       0.82\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     96/100      2.38G     0.5462     0.5146      1.136          6        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:06<00:00,  2.31s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:24<00:00,  3.53s/it]\n","                   all        217        254      0.905      0.942      0.962      0.818\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     97/100      2.39G     0.5282     0.4965      1.127          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:05<00:00,  2.28s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.39s/it]\n","                   all        217        254      0.909      0.927      0.961      0.819\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     98/100      2.41G     0.5296     0.4902      1.116          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:02<00:00,  2.23s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.05s/it]\n","                   all        217        254      0.921      0.918      0.962      0.824\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","     99/100      2.36G     0.5302     0.5019      1.127          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:03<00:00,  2.24s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:23<00:00,  3.31s/it]\n","                   all        217        254      0.913      0.926      0.962       0.82\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","    100/100      2.36G     0.5378      0.511      1.123          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [02:00<00:00,  2.18s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:20<00:00,  2.90s/it]\n","                   all        217        254      0.906      0.934      0.962      0.817\n","\n","100 epochs completed in 4.343 hours.\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úó Training failed: 'str' object has no attribute '__module__'\n","\n","Full error traceback:\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/tmp/ipython-input-3783686911.py\", line 16, in <cell line: 0>\n","    results = model.train(**TRAINING_CONFIG)\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\", line 341, in train\n","    self.trainer.train()\n","  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 192, in train\n","    self._do_train(world_size)\n","  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 413, in _do_train\n","    self.final_eval()\n","  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 558, in final_eval\n","    strip_optimizer(f)  # strip optimizers\n","    ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/utils/torch_utils.py\", line 445, in strip_optimizer\n","    x = torch.load(f, map_location=torch.device('cpu'))\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1521, in load\n","    return _load(\n","           ^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 2119, in _load\n","    result = unpickler.load()\n","             ^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/_weights_only_unpickler.py\", line 337, in load\n","    elif full_path in _get_user_allowed_globals():\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/_weights_only_unpickler.py\", line 144, in _get_user_allowed_globals\n","    module, name = f.__module__, f.__qualname__\n","                   ^^^^^^^^^^^^\n","AttributeError: 'str' object has no attribute '__module__'. Did you mean: '__mod__'?\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'str' object has no attribute '__module__'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3783686911.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mTRAINING_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Calculate duration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_ddp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    411\u001b[0m             LOGGER.info(f'\\n{epoch - self.start_epoch + 1} epochs completed in '\n\u001b[1;32m    412\u001b[0m                         f'{(time.time() - self.train_time_start) / 3600:.3f} hours.')\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mfinal_eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0mstrip_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# strip optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                     \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nValidating {f}...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/utils/torch_utils.py\u001b[0m in \u001b[0;36mstrip_optimizer\u001b[0;34m(f, s)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \"\"\"\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'model'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Skipping {f}, not a valid Ultralytics model.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                         return _load(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                             \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                             \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2117\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2119\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2120\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfull_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_allowed_globals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_allowed_globals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                 \u001b[0;32melif\u001b[0m \u001b[0mfull_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_user_allowed_globals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_user_allowed_globals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                 elif full_path in (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36m_get_user_allowed_globals\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{module}.{name}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '__module__'"]}],"source":["import time\n","from datetime import datetime\n","\n","print('='*70)\n","print('STARTING TRAINING')\n","print('='*70)\n","print(f'Start time: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n","print(f'Expected duration: ~3-4 hours (100 epochs on T4 GPU)')\n","print('='*70)\n","print('\\n‚è≥ Training in progress... Please wait.\\n')\n","\n","start_time = time.time()\n","\n","try:\n","    # Start training\n","    results = model.train(**TRAINING_CONFIG)\n","\n","    # Calculate duration\n","    duration = time.time() - start_time\n","    hours = int(duration // 3600)\n","    minutes = int((duration % 3600) // 60)\n","\n","    print('\\n' + '='*70)\n","    print('‚úì TRAINING COMPLETED SUCCESSFULLY!')\n","    print('='*70)\n","    print(f'End time: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n","    print(f'Total duration: {hours}h {minutes}m')\n","    print(f'Results saved to: {RESULTS_DIR}/{TRAINING_CONFIG[\"name\"]}')\n","    print('='*70)\n","\n","except KeyboardInterrupt:\n","    print('\\n‚ö† Training interrupted by user')\n","    print('  Partial results may be available in the results directory')\n","\n","except torch.cuda.OutOfMemoryError:\n","    print('\\n‚úó CUDA Out of Memory Error!')\n","    print('  Solutions:')\n","    print('  1. Reduce batch size: Change TRAINING_CONFIG[\"batch\"] to 8 or 4')\n","    print('  2. Reduce image size: Change TRAINING_CONFIG[\"imgsz\"] to 416')\n","    print('  3. Enable cache=False (already set)')\n","    print('  4. Restart runtime and try again')\n","    raise\n","\n","except Exception as e:\n","    print(f'\\n‚úó Training failed: {e}')\n","    import traceback\n","    print('\\nFull error traceback:')\n","    traceback.print_exc()\n","    raise"]},{"cell_type":"markdown","id":"8575d7e1","metadata":{"id":"8575d7e1"},"source":["## üìä Step 11: Validate Trained Model\n","\n","Run validation to get final metrics."]},{"cell_type":"code","execution_count":22,"id":"64595cff","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64595cff","executionInfo":{"status":"ok","timestamp":1762513968399,"user_tz":-360,"elapsed":42935,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"0d9be345-36c1-4ee0-ea51-2842473a7fbc"},"outputs":[{"output_type":"stream","name":"stderr","text":["Ultralytics YOLOv8.0.200 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n"]},{"output_type":"stream","name":"stdout","text":["======================================================================\n","MODEL VALIDATION\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1CA2eiw-QqvOG-A8SZwgNxgr29ai5UAcR/YOLOv8 Traffic/dataset/labels/val.cache... 217 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 217/217 [00:00<?, ?it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:33<00:00,  2.42s/it]\n","                   all        217        254      0.907      0.938      0.962       0.82\n","               bicycle        217         17      0.945          1      0.995       0.87\n","                   bus        217         29      0.896      0.893      0.942      0.834\n","                   car        217         16          1      0.996      0.995      0.873\n","                   cng        217         34      0.883      0.971       0.96      0.831\n","                  auto        217         36       0.91      0.843      0.953       0.84\n","                  bike        217         25      0.904          1      0.995      0.813\n","              rickshaw        217         32      0.859      0.952      0.937       0.77\n","                 truck        217         32      0.931      0.938      0.962       0.81\n","                   van        217         33      0.837      0.848      0.922      0.737\n","Speed: 1.4ms preprocess, 10.8ms inference, 0.0ms loss, 1.8ms postprocess per image\n","Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","VALIDATION RESULTS\n","======================================================================\n","mAP50: 0.9623 (96.23%)\n","mAP50-95: 0.8197 (81.97%)\n","Precision: 0.9073 (90.73%)\n","Recall: 0.9380 (93.80%)\n","======================================================================\n","\n","‚úì Validation completed successfully!\n"]}],"source":["print('='*70)\n","print('MODEL VALIDATION')\n","print('='*70)\n","\n","try:\n","    # Run validation with the data config\n","    # CRITICAL: Must pass data argument since training already completed\n","    val_results = model.val(data=DATA_YAML)\n","\n","    print('\\n' + '='*70)\n","    print('VALIDATION RESULTS')\n","    print('='*70)\n","    print(f'mAP50: {val_results.box.map50:.4f} ({val_results.box.map50*100:.2f}%)')\n","    print(f'mAP50-95: {val_results.box.map:.4f} ({val_results.box.map*100:.2f}%)')\n","    print(f'Precision: {val_results.box.mp:.4f} ({val_results.box.mp*100:.2f}%)')\n","    print(f'Recall: {val_results.box.mr:.4f} ({val_results.box.mr*100:.2f}%)')\n","    print('='*70)\n","\n","    print('\\n‚úì Validation completed successfully!')\n","\n","except Exception as e:\n","    print(f'‚úó Validation failed: {e}')\n","    import traceback\n","    traceback.print_exc()\n"]},{"cell_type":"markdown","id":"c4406071","metadata":{"id":"c4406071"},"source":["## üíæ Step 12: Copy Results to Google Drive\n","\n","Save all training results to your Google Drive for safekeeping."]},{"cell_type":"code","execution_count":25,"id":"27731462","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"27731462","executionInfo":{"status":"ok","timestamp":1762514063878,"user_tz":-360,"elapsed":809,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"32f15077-b003-4e36-953d-0ea3ac8d334b"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","COPYING RESULTS TO GOOGLE DRIVE\n","======================================================================\n","\n","Source: yolov8_cbam_training/bd_vehicles_run\n","Destination: /content/drive/MyDrive/YOLOv8 Traffic/training_results/bd_vehicles_run\n","\n","Copying files...\n","‚úì Weights copied (2 files)\n","‚úì Copied 4 result files\n","‚úì Copied 0 validation prediction images\n","\n","======================================================================\n","‚úì ALL RESULTS COPIED TO GOOGLE DRIVE!\n","======================================================================\n","üìÅ Location: /content/drive/MyDrive/YOLOv8 Traffic/training_results/bd_vehicles_run\n","\n","üìä Key files:\n","  - Best model: /content/drive/MyDrive/YOLOv8 Traffic/training_results/bd_vehicles_run/weights/best.pt\n","  - Last checkpoint: /content/drive/MyDrive/YOLOv8 Traffic/training_results/bd_vehicles_run/weights/last.pt\n","  - Training curves: /content/drive/MyDrive/YOLOv8 Traffic/training_results/bd_vehicles_run/results.png\n","  - Confusion matrix: /content/drive/MyDrive/YOLOv8 Traffic/training_results/bd_vehicles_run/confusion_matrix.png\n","======================================================================\n"]}],"source":["import shutil\n","from pathlib import Path\n","\n","print('='*70)\n","print('COPYING RESULTS TO GOOGLE DRIVE')\n","print('='*70)\n","\n","try:\n","    # Try multiple possible source locations\n","    possible_sources = [\n","        f'runs/detect/{TRAINING_CONFIG[\"project\"]}/{TRAINING_CONFIG[\"name\"]}',\n","        f'{TRAINING_CONFIG[\"project\"]}/{TRAINING_CONFIG[\"name\"]}',\n","        f'yolov8_cbam_training/bd_vehicles_run',\n","        'runs/detect/train',\n","        'runs/detect/train2',\n","        'runs/detect/train3',\n","    ]\n","\n","    # Find the actual results directory\n","    source_dir = None\n","    for path in possible_sources:\n","        if Path(path).exists() and (Path(path) / 'weights').exists():\n","            source_dir = path\n","            break\n","\n","    if source_dir is None:\n","        # List all directories in runs/detect/ to help debug\n","        print('‚ö† Could not find results directory automatically.')\n","        print('\\nSearching for training results...')\n","\n","        runs_detect = Path('runs/detect')\n","        if runs_detect.exists():\n","            all_runs = sorted(runs_detect.glob('*'), key=lambda p: p.stat().st_mtime, reverse=True)\n","            print(f'\\nFound {len(all_runs)} run(s) in runs/detect/:')\n","            for i, run_dir in enumerate(all_runs[:5], 1):\n","                has_weights = (run_dir / 'weights').exists()\n","                print(f'  {i}. {run_dir.name} {\"‚úì (has weights)\" if has_weights else \"‚úó (no weights)\"}')\n","\n","            # Use the most recent run with weights\n","            for run_dir in all_runs:\n","                if (run_dir / 'weights').exists():\n","                    source_dir = str(run_dir)\n","                    print(f'\\n‚úì Using most recent run: {source_dir}')\n","                    break\n","\n","        if source_dir is None:\n","            raise FileNotFoundError('No training results found. Make sure training completed successfully.')\n","\n","    # Destination in Google Drive\n","    run_name = Path(source_dir).name\n","    dest_dir = f'{DRIVE_PROJECT_PATH}/training_results/{run_name}'\n","\n","    # Create destination directory\n","    os.makedirs(dest_dir, exist_ok=True)\n","\n","    print(f'\\nSource: {source_dir}')\n","    print(f'Destination: {dest_dir}')\n","    print('\\nCopying files...')\n","\n","    # Copy weights folder\n","    weights_src = Path(source_dir) / 'weights'\n","    weights_dst = Path(dest_dir) / 'weights'\n","    if weights_src.exists():\n","        shutil.copytree(weights_src, weights_dst, dirs_exist_ok=True)\n","        # Count weight files\n","        weight_files = list(weights_dst.glob('*.pt'))\n","        print(f'‚úì Weights copied ({len(weight_files)} files)')\n","    else:\n","        print(f'‚ö† Weights folder not found at {weights_src}')\n","\n","    # Copy important files\n","    files_to_copy = [\n","        'results.png',\n","        'results.csv',\n","        'confusion_matrix.png',\n","        'confusion_matrix_normalized.png',\n","        'F1_curve.png',\n","        'PR_curve.png',\n","        'P_curve.png',\n","        'R_curve.png',\n","        'labels.jpg',\n","        'labels_correlogram.jpg',\n","        'args.yaml',\n","    ]\n","\n","    copied_count = 0\n","    for filename in files_to_copy:\n","        src = Path(source_dir) / filename\n","        dst = Path(dest_dir) / filename\n","        if src.exists():\n","            shutil.copy2(src, dst)\n","            copied_count += 1\n","\n","    print(f'‚úì Copied {copied_count} result files')\n","\n","    # Copy validation batch predictions\n","    val_batches = list(Path(source_dir).glob('val_batch*.jpg'))\n","    copied_batches = 0\n","    for val_batch in val_batches[:6]:  # Copy first 6 only\n","        dst = Path(dest_dir) / val_batch.name\n","        shutil.copy2(val_batch, dst)\n","        copied_batches += 1\n","    print(f'‚úì Copied {copied_batches} validation prediction images')\n","\n","    print('\\n' + '='*70)\n","    print('‚úì ALL RESULTS COPIED TO GOOGLE DRIVE!')\n","    print('='*70)\n","    print(f'üìÅ Location: {dest_dir}')\n","    print(f'\\nüìä Key files:')\n","    print(f'  - Best model: {dest_dir}/weights/best.pt')\n","    print(f'  - Last checkpoint: {dest_dir}/weights/last.pt')\n","    print(f'  - Training curves: {dest_dir}/results.png')\n","    print(f'  - Confusion matrix: {dest_dir}/confusion_matrix.png')\n","    print('='*70)\n","\n","except Exception as e:\n","    print(f'\\n‚ö† Failed to copy results: {e}')\n","    print(f'\\nTrying to list available directories...')\n","    try:\n","        runs_detect = Path('runs/detect')\n","        if runs_detect.exists():\n","            all_dirs = list(runs_detect.glob('*'))\n","            print(f'Directories in runs/detect/: {[d.name for d in all_dirs]}')\n","    except:\n","        pass\n","    import traceback\n","    traceback.print_exc()\n"]},{"cell_type":"markdown","id":"906c69e8","metadata":{"id":"906c69e8"},"source":["## üì• Step 13: Download Best Model (Optional)\n","\n","Download the best model weights to your local computer."]},{"cell_type":"code","execution_count":null,"id":"80ad5029","metadata":{"id":"80ad5029","executionInfo":{"status":"aborted","timestamp":1762513239135,"user_tz":-360,"elapsed":15533057,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}}},"outputs":[],"source":["from google.colab import files\n","\n","best_model_path = f'{dest_dir}/weights/best.pt'\n","if os.path.exists(best_model_path):\n","    print(f'Downloading: {best_model_path}')\n","    files.download(best_model_path)\n","    print('‚úì Download started!')\n","else:\n","    print(f'‚úó Model not found at: {best_model_path}')\n","\n","print('Uncomment the code above to download the best model')\n","print(f'Or access it from Google Drive: {dest_dir}/weights/best.pt')"]},{"cell_type":"markdown","id":"8461746a","metadata":{"id":"8461746a"},"source":["## üéØ Step 14: Test Inference (Optional)\n","\n","Run inference on validation images to see predictions."]},{"cell_type":"code","execution_count":null,"id":"02d962e2","metadata":{"id":"02d962e2","executionInfo":{"status":"aborted","timestamp":1762496747989,"user_tz":-360,"elapsed":59834,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}}},"outputs":[],"source":["print('='*70)\n","print('TEST INFERENCE')\n","print('='*70)\n","\n","# Load best model\n","best_model_path = f'{dest_dir}/weights/best.pt'\n","\n","if os.path.exists(best_model_path):\n","    print(f'Loading best model from: {best_model_path}')\n","    best_model = YOLO(best_model_path)\n","    print('‚úì Model loaded')\n","\n","    # Uncomment to run inference on validation set\n","    # val_images_path = f'{DATASET_PATH}/{data_config[\"val\"]}'\n","    #\n","    # print(f'\\nRunning inference on: {val_images_path}')\n","    # results = best_model.predict(\n","    #     source=val_images_path,\n","    #     save=True,\n","    #     save_txt=True,\n","    #     conf=0.25,\n","    #     iou=0.45,\n","    #     project='/content/predictions',\n","    #     name='val_predictions',\n","    #     max_det=300,\n","    #     augment=False,\n","    #     agnostic_nms=False,\n","    # )\n","    #\n","    # print('‚úì Inference completed')\n","    # print('üìÅ Predictions saved to: /content/predictions/val_predictions')\n","\n","    print('\\nUncomment the code above to run inference on validation set')\n","else:\n","    print(f'‚úó Best model not found at: {best_model_path}')\n","    print('  Make sure training completed successfully')"]},{"cell_type":"markdown","id":"c5b17138","metadata":{"id":"c5b17138"},"source":["## üéâ Training Complete!\n","\n","### üìä What You Have:\n","\n","1. **Trained Model**: YOLOv8n with CBAM attention mechanism\n","2. **Results Saved**:\n","   - Google Drive: `{dest_dir}`\n","   - Best weights: `weights/best.pt`\n","   - Training curves: `results.png`\n","   - Confusion matrix: `confusion_matrix.png`\n","\n","### üìà Expected Performance:\n","- **mAP50**: 98-99% (excellent!)\n","- **mAP50-95**: 89-91% (very good!)\n","- **Precision & Recall**: 95-96%\n","\n","### üöÄ Next Steps:\n","\n","1. **Review Results**:\n","   - Check `results.png` for training curves\n","   - Review `confusion_matrix.png` for per-class performance\n","\n","2. **Use Your Model**:\n","   ```python\n","   from ultralytics import YOLO\n","   model = YOLO('path/to/best.pt')\n","   results = model('path/to/image.jpg')\n","   ```\n","\n","3. **Export for Deployment**:\n","   ```python\n","   model.export(format='onnx')  # For cross-platform\n","   model.export(format='engine')  # For NVIDIA TensorRT\n","   ```\n","\n","### üí° Tips:\n","- Results are saved in your Google Drive\n","- You can disconnect from Colab now\n","- To resume training, load `weights/last.pt` and set `resume=True`\n","\n","---\n","\n","**Thank you for using YOLOv8-CBAM! üôè**\n","\n","For questions or issues, check the documentation in the `Attention` folder."]},{"cell_type":"code","source":[],"metadata":{"id":"YjzBZfOwAhN8"},"id":"YjzBZfOwAhN8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"3b3604d3","executionInfo":{"status":"ok","timestamp":1762514002277,"user_tz":-360,"elapsed":4533,"user":{"displayName":"Sunzil Khandaker","userId":"04273097575844328186"}},"outputId":"97692d0c-a57c-451d-badc-0e535ca3e9b9"},"source":["import shutil\n","from google.colab import files\n","import os\n","\n","source_folder = '/content/yolov8_cbam_training'\n","output_filename = 'yolov8_attention'\n","zip_file_path = f'/content/{output_filename}.zip'\n","\n","if os.path.exists(source_folder):\n","    print(f'Compressing folder: {source_folder}...')\n","    shutil.make_archive(output_filename, 'zip', source_folder)\n","    print(f'‚úì Folder compressed to: {zip_file_path}')\n","    print('Initiating download...')\n","    files.download(zip_file_path)\n","    print('‚úì Download initiated for the zipped folder!')\n","else:\n","    print(f'‚úó Error: Folder not found at {source_folder}. Please ensure the path is correct.')"],"id":"3b3604d3","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Compressing folder: /content/yolov8_cbam_training...\n","‚úì Folder compressed to: /content/yolov8_attention.zip\n","Initiating download...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_dfbe8725-d864-478d-a858-82599255a350\", \"yolov8_attention.zip\", 70919248)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úì Download initiated for the zipped folder!\n"]}]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}